{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import tensorflow as tf\n\ntf.enable_eager_execution()\n\nimport numpy as np\n\nimport os\nimport time"
    },
    {
      "cell_type": "markdown",
      "source": "# 1. prepare data\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "download file",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "path_to_file \u003d tf.keras.utils.get_file(\u0027shakespeare.txt\u0027,\n                                       \u0027https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\u0027)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "text \u003d open(path_to_file).read()\nprint(\u0027Length of text: {} characters\u0027.format(len(text)))\nprint(text[:100])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "vocab \u003d sorted(set(text))\nprint(\u0027{} unique characters\u0027.format(len(vocab)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": "char2idx \u003d {u: i for i, u in enumerate(vocab)}\n\nidx2char \u003d np.array(vocab)\ntext_as_int \u003d np.array([char2idx[c] for c in text])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% create char array and int(represent char) array\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u0027\\n\u0027   ---\u003e    0\n\u0027 \u0027    ---\u003e    1\n\u0027!\u0027    ---\u003e    2\n\u0027$\u0027    ---\u003e    3\n\u0027\u0026\u0027    ---\u003e    4\n\"\u0027\"    ---\u003e    5\n\u0027,\u0027    ---\u003e    6\n\u0027-\u0027    ---\u003e    7\n\u0027.\u0027    ---\u003e    8\n\u00273\u0027    ---\u003e    9\n\u0027:\u0027    ---\u003e   10\n\u0027;\u0027    ---\u003e   11\n\u0027?\u0027    ---\u003e   12\n\u0027A\u0027    ---\u003e   13\n\u0027B\u0027    ---\u003e   14\n\u0027C\u0027    ---\u003e   15\n\u0027D\u0027    ---\u003e   16\n\u0027E\u0027    ---\u003e   17\n\u0027F\u0027    ---\u003e   18\n\u0027G\u0027    ---\u003e   19\nFirst Citizen ---- characters mapped to int ---- \u003e [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# note: zip(list, range(n))\nfor char, _ in zip(char2idx, range(20)):\n    print(\u0027{:6s} ---\u003e {:4d}\u0027.format(repr(char), char2idx[char]))\nprint(\u0027{} ---- characters mapped to int ---- \u003e {}\u0027.format(text[:13], text_as_int[:13]))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# step 2: train\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": "# suppose text \u003d \"hello\"\n# train sample: hell, target sample: ello\nseq_length \u003d 10\nchunks \u003d tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length + 1, drop_remainder\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u0027First Citiz\u0027\n\u0027en:\\nBefore \u0027\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# note: repr()\nfor item in chunks.take(2):\n    print(repr(\u0027\u0027.join(idx2char[item.numpy()])))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "def split_input_target(chunk):\n    input_text \u003d chunk[:-1]\n    target_text \u003d chunk[1:]\n    return input_text, target_text\n\ndataset \u003d chunks.map(split_input_target)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Input data:  \u0027First Citi\u0027\nTarget data:  \u0027irst Citiz\u0027\nStep    0\n  input: 18 (\u0027F\u0027)\n  expected output: 47 (\u0027i\u0027)\nStep    1\n  input: 47 (\u0027i\u0027)\n  expected output: 56 (\u0027r\u0027)\nStep    2\n  input: 56 (\u0027r\u0027)\n  expected output: 57 (\u0027s\u0027)\nStep    3\n  input: 57 (\u0027s\u0027)\n  expected output: 58 (\u0027t\u0027)\nStep    4\n  input: 58 (\u0027t\u0027)\n  expected output: 1 (\u0027 \u0027)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "for input_example, target_example in dataset.take(1):\n    print(\u0027Input data: \u0027, repr(\u0027\u0027.join(idx2char[input_example.numpy()])))\n    print(\u0027Target data: \u0027, repr(\u0027\u0027.join(idx2char[target_example.numpy()])))\n    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n        print(\u0027Step {:4d}\u0027.format(i))\n        print(\u0027  input: {} ({:s})\u0027.format(input_idx, repr(idx2char[input_idx])))\n        print(\u0027  expected output: {} ({:s})\u0027.format(target_idx, repr(idx2char[target_idx])))\n        ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": "BATCH_SIZE \u003d 64    \nBUFFER_SIZE \u003d 10000\ndataset \u003d dataset.shuffle(BUFFER_SIZE).batch(   BATCH_SIZE, drop_remainder\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": "class Model(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, units):\n    super(Model, self).__init__()\n    self.units \u003d units\n\n    self.embedding \u003d tf.keras.layers.Embedding(vocab_size, embedding_dim)\n\n    if tf.test.is_gpu_available():\n      self.gru \u003d tf.keras.layers.CuDNNGRU(self.units,\n                                          return_sequences\u003dTrue,\n                                          recurrent_initializer\u003d\u0027glorot_uniform\u0027,\n                                          stateful\u003dTrue)\n    else:\n      self.gru \u003d tf.keras.layers.GRU(self.units,\n                                     return_sequences\u003dTrue,\n                                     recurrent_activation\u003d\u0027sigmoid\u0027,\n                                     recurrent_initializer\u003d\u0027glorot_uniform\u0027,\n                                     stateful\u003dTrue)\n\n    self.fc \u003d tf.keras.layers.Dense(vocab_size)\n\n  def call(self, x):\n    embedding \u003d self.embedding(x)\n\n    # output at every time step\n    # output shape \u003d\u003d (batch_size, seq_length, hidden_size)\n    output \u003d self.gru(embedding)\n\n    # The dense layer will output predictions for every time_steps(seq_length)\n    # output shape after the dense layer \u003d\u003d (seq_length * batch_size, vocab_size)\n    prediction \u003d self.fc(output)\n\n    # states will be used to pass at every step to the model while training\n    return prediction",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": "vocab_size \u003d len(vocab)\nembedding_dim \u003d 256\nunits \u003d 1024\nmodel \u003d Model(vocab_size, embedding_dim, units)\noptimizer \u003d tf.train.AdamOptimizer()\ndef loss_function(real, preds):\n    return tf.losses.sparse_softmax_cross_entropy(labels\u003dreal, logits\u003dpreds)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nembedding (Embedding)        multiple                  16640     \n_________________________________________________________________\ngru (GRU)                    multiple                  3935232   \n_________________________________________________________________\ndense (Dense)                multiple                  66625     \n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nTotal params: 4,018,497\nTrainable params: 4,018,497\nNon-trainable params: 0\n_________________________________________________________________\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\nmodel.summary()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1746\n",
            "Epoch 1 Batch 10 Loss 3.9664\n",
            "Epoch 1 Batch 20 Loss 3.9042\n",
            "Epoch 1 Batch 30 Loss 3.4811\n",
            "Epoch 1 Batch 40 Loss 3.0365\n",
            "Epoch 1 Batch 50 Loss 2.9001\n",
            "Epoch 1 Batch 60 Loss 2.7335\n",
            "Epoch 1 Batch 70 Loss 2.6124\n",
            "Epoch 1 Batch 80 Loss 2.5212\n",
            "Epoch 1 Batch 90 Loss 2.5978\n",
            "Epoch 1 Batch 100 Loss 2.4509\n",
            "Epoch 1 Batch 110 Loss 2.3718\n",
            "Epoch 1 Batch 120 Loss 2.3378\n",
            "Epoch 1 Batch 130 Loss 2.4749\n",
            "Epoch 1 Batch 140 Loss 2.3511\n",
            "Epoch 1 Batch 150 Loss 2.3236\n",
            "Epoch 1 Batch 160 Loss 2.4248\n",
            "Epoch 1 Batch 170 Loss 2.3409\n",
            "Epoch 1 Batch 180 Loss 2.4305\n",
            "Epoch 1 Batch 190 Loss 2.4623\n",
            "Epoch 1 Batch 200 Loss 2.3532\n",
            "Epoch 1 Batch 210 Loss 2.2522\n",
            "Epoch 1 Batch 220 Loss 2.2653\n",
            "Epoch 1 Batch 230 Loss 2.2788\n",
            "Epoch 1 Batch 240 Loss 2.2386\n",
            "Epoch 1 Batch 250 Loss 2.3055\n",
            "Epoch 1 Batch 260 Loss 2.0711\n",
            "Epoch 1 Batch 270 Loss 2.2431\n",
            "Epoch 1 Batch 280 Loss 2.0654\n",
            "Epoch 1 Batch 290 Loss 2.2164\n",
            "Epoch 1 Batch 300 Loss 2.1990\n",
            "Epoch 1 Batch 310 Loss 2.1690\n",
            "Epoch 1 Batch 320 Loss 2.1502\n",
            "Epoch 1 Batch 330 Loss 2.1698\n",
            "Epoch 1 Batch 340 Loss 2.0993\n",
            "Epoch 1 Batch 350 Loss 2.1859\n",
            "Epoch 1 Batch 360 Loss 2.1189\n",
            "Epoch 1 Batch 370 Loss 2.1761\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "checkpoint_dir \u003d \u0027./training_checkpoints\u0027\ncheckpoint_prefix \u003d os.path.join(checkpoint_dir, \u0027ckpt\u0027)\nepochs \u003d 5\nfor epoch in range(epochs):\n    start \u003d time.time()\n\n    hidden \u003d model.reset_states()\n\n    for (batch, (inp,target)) in enumerate(dataset):\n        with tf.GradientTape() as tape:\n            predictions \u003d model(inp)\n            loss \u003d loss_function(target, predictions)\n        grads \u003d tape.gradient(loss, model.variables)\n        optimizer.apply_gradients(zip(grads, model.variables))\n\n        if batch % 10 \u003d\u003d 0:\n            print(\u0027Epoch {} Batch {} Loss {:.4f}\u0027.format(epoch+1, batch, loss))\n    if (epoch + 1) % 5 \u003d\u003d 0:\n        model.save_weights(checkpoint_prefix)\n    print(\u0027Epoch {} Loss {:.4f}\u0027.format(epoch+1, loss))\n    print(\u0027Time taken for 1 epoch {} sec\\n\u0027.format(time.time() - start))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "model.save_weights(checkpoint_prefix)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## generate text",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "model \u003d Model(vocab_size, embedding_dim, units)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))\n\nnum_generate \u003d 1000\nstart_string \u003d \u0027Q\u0027\n\ninput_eval \u003d [char2idx[s] for s in start_string]\ninput_eval \u003d tf.expand_dims(input_eval, 0)\n\ntext_generated \u003d []\n\ntemperature \u003d 1.0\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "model.reset_states()\nfor i in range(num_generate):\n    predictions \u003d model(input_eval)\n    predictions \u003d tf.squeeze(predictions, 0)\n    \n    predictions \u003d predictions / temperature\n    predicted_id \u003d tf.multinomial(predictions, num_samples\u003d1)[-1,0].numpy()\n    \n    input_eval \u003d tf.expand_dims([predicted_id], 0)\n    \n    text_generated.append(idx2char[predicted_id])\n\nprint(start_string + \u0027\u0027.join(text_generated))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language": "python",
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}